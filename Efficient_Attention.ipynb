{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrigankpawagi/EfficientAttention-TermPaper/blob/main/Efficient_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT2uXIBELQ0f"
      },
      "source": [
        "Contributors: Rohit Jorige, Mrigank Pawagi, Nagasai, Sahil Chaudhary\n",
        "\n",
        "**Efficient Attention**\n",
        "\n",
        "The dominant sequence transduction models are based on complex recurrent or convulational neural networks that include an encoder and decoder. Vashwani et. al. proposed new simple network architecture, the Transformer based solely on attention mechanism dispensing with recurrence and convulations entirely. There is a scope of improvement on the mechanism proposed by vaswant et. al.\n",
        "\n",
        "Attention is a very important mechanism introduce in the field of LLMs and understanding the improvement in attention is very important. The time complexity of simple attention mechanism is quadratic in nature and its a problematic time complexity for a very large number of data. Thus, Approv vyas et. al. proposed a linear transformer for the same introducing kernels to similarity function.\n",
        "\n",
        "This Google Collab includes the experiments done around\n",
        "1. Attention\n",
        "2. Linear Transformer\n",
        "3. Improving attention using LoRA\n",
        "4. (And others???)\n",
        "\n",
        "We have tried various experiments around MNIST, cifar-10 and compare the time required to perform forward and backward pass for variety of sequence lengths and batch sizes.\n",
        "\n",
        "**Credits**\n",
        "1. https://github.com/idiap/fast-transformers?tab=readme-ov-file\n",
        "2. https://github.com/lucidrains/linear-attention-transformer\n",
        "3. https://github.com/EugenHotaj/pytorch-generative/blob/master/pytorch_generative/nn/attention.py\n",
        "4. https://github.com/davidsvy/cosformer-pytorch\n",
        "5. https://github.com/kyle-gao/TF_Transformer\n",
        "\n",
        "**Papers Followed:**\n",
        "1. Attention is all you need: https://arxiv.org/pdf/1706.03762.pdf\n",
        "2. Transformers are RNNs: https: //arxiv.org/pdf/2006.16236.pdf\n",
        "3. LoRA: https://arxiv.org/pdf/2106.09685.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0mo0btDOnOs"
      },
      "source": [
        "# Installation and First Steps\n",
        "The installation is directly from PyPI. This will take several minutes since it compiles several custom CUDA kernels, not only for linear autoregressive attention. Maybe grab a coffee (if you are into these things)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSW2PtZoLPsv",
        "outputId": "bdeeddce-f6b8-47b2-cd3e-3db460e11bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting pytorch-fast-transformers\n",
            "  Downloading pytorch-fast-transformers-0.4.0.tar.gz (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.6/93.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command python setup.py egg_info\n",
            "  No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/SOURCES.txt'\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:500: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/SOURCES.txt'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/SOURCES.txt'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch-fast-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pytorch-fast-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch-fast-transformers) (1.3.0)\n",
            "Building wheels for collected packages: pytorch-fast-transformers\n",
            "  Running command python setup.py bdist_wheel\n",
            "  No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "  running bdist_wheel\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:500: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib.linux-x86_64-cpython-310\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers\n",
            "  copying fast_transformers/utils.py -> build/lib.linux-x86_64-cpython-310/fast_transformers\n",
            "  copying fast_transformers/masking.py -> build/lib.linux-x86_64-cpython-310/fast_transformers\n",
            "  copying fast_transformers/weight_mapper.py -> build/lib.linux-x86_64-cpython-310/fast_transformers\n",
            "  copying fast_transformers/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers\n",
            "  copying fast_transformers/transformers.py -> build/lib.linux-x86_64-cpython-310/fast_transformers\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/local_product\n",
            "  copying fast_transformers/local_product/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/local_product\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/events\n",
            "  copying fast_transformers/events/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/events\n",
            "  copying fast_transformers/events/filters.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/events\n",
            "  copying fast_transformers/events/event.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/events\n",
            "  copying fast_transformers/events/event_dispatcher.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/events\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/hashing\n",
            "  copying fast_transformers/hashing/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/hashing\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/full_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/improved_clustered_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/local_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/linear_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/reformer_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/conditional_full_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/causal_linear_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/exact_topk_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/attention_layer.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/clustered_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/improved_clustered_causal_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent\n",
            "  copying fast_transformers/recurrent/_utils.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent\n",
            "  copying fast_transformers/recurrent/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent\n",
            "  copying fast_transformers/recurrent/transformers.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/causal_product\n",
            "  copying fast_transformers/causal_product/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/causal_product\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/clustering\n",
            "  copying fast_transformers/clustering/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/clustering\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product\n",
            "  copying fast_transformers/sparse_product/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate\n",
            "  copying fast_transformers/aggregate/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/attention_registry\n",
            "  copying fast_transformers/attention_registry/spec.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention_registry\n",
            "  copying fast_transformers/attention_registry/registry.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention_registry\n",
            "  copying fast_transformers/attention_registry/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention_registry\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/builders\n",
            "  copying fast_transformers/builders/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/builders\n",
            "  copying fast_transformers/builders/base.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/builders\n",
            "  copying fast_transformers/builders/transformer_builders.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/builders\n",
            "  copying fast_transformers/builders/attention_builders.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/builders\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/feature_maps\n",
            "  copying fast_transformers/feature_maps/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/feature_maps\n",
            "  copying fast_transformers/feature_maps/base.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/feature_maps\n",
            "  copying fast_transformers/feature_maps/fourier_features.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/feature_maps\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention\n",
            "  copying fast_transformers/recurrent/attention/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention\n",
            "  copying fast_transformers/recurrent/attention/self_attention/full_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention\n",
            "  copying fast_transformers/recurrent/attention/self_attention/linear_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention\n",
            "  copying fast_transformers/recurrent/attention/self_attention/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention\n",
            "  copying fast_transformers/recurrent/attention/self_attention/attention_layer.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying fast_transformers/recurrent/attention/cross_attention/full_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying fast_transformers/recurrent/attention/cross_attention/linear_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying fast_transformers/recurrent/attention/cross_attention/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying fast_transformers/recurrent/attention/cross_attention/attention_layer.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/clustering/hamming\n",
            "  copying fast_transformers/clustering/hamming/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/clustering/hamming\n",
            "  running build_ext\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:415: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "    warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:425: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "  building 'fast_transformers.hashing.hash_cpu' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/hashing\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/hashing/hash_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=hash_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.aggregate.aggregate_cpu' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/aggregate/aggregate_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=aggregate_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.clustering.hamming.cluster_cpu' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/clustering\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/clustering/hamming\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/clustering/hamming/cluster_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=cluster_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.sparse_product.sparse_product_cpu' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/sparse_product/sparse_product_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=sparse_product_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.sparse_product.clustered_sparse_product_cpu' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/sparse_product/clustered_sparse_product_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/clustered_sparse_product_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=clustered_sparse_product_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/clustered_sparse_product_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product/clustered_sparse_product_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.causal_product.causal_product_cpu' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/causal_product\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/causal_product/causal_product_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/causal_product/causal_product_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=causal_product_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/causal_product/causal_product_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/causal_product/causal_product_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.local_product.local_product_cpu' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/local_product\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/local_product/local_product_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/local_product/local_product_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=local_product_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/local_product/local_product_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/local_product/local_product_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.hashing.hash_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/hashing/hash_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=hash_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.aggregate.aggregate_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/aggregate/aggregate_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=aggregate_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.aggregate.clustered_aggregate_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/aggregate/clustered_aggregate_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate/clustered_aggregate_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=clustered_aggregate_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  fast_transformers/aggregate/clustered_aggregate_cuda.cu(43): warning #177-D: variable \"e_idx\" was declared but never referenced\n",
            "        int e_idx = threadIdx.y;\n",
            "            ^\n",
            "\n",
            "  Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate/clustered_aggregate_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate/clustered_aggregate_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.clustering.hamming.cluster_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/clustering/hamming/cluster_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=cluster_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.sparse_product.sparse_product_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/sparse_product/sparse_product_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=sparse_product_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  fast_transformers/sparse_product/sparse_product_cuda.cu(272): warning #177-D: variable \"max_threads\" was declared but never referenced\n",
            "        int max_threads = 1024;\n",
            "            ^\n",
            "\n",
            "  Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.sparse_product.clustered_sparse_product_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/sparse_product/clustered_sparse_product_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/clustered_sparse_product_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=clustered_sparse_product_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/clustered_sparse_product_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product/clustered_sparse_product_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.causal_product.causal_product_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/causal_product/causal_product_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/causal_product/causal_product_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=causal_product_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/causal_product/causal_product_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/causal_product/causal_product_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.local_product.local_product_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/local_product/local_product_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/local_product/local_product_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=local_product_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/local_product/local_product_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/local_product/local_product_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "  !!\n",
            "\n",
            "          ********************************************************************************\n",
            "          Please avoid running ``setup.py`` directly.\n",
            "          Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "          other standards-based tools.\n",
            "\n",
            "          See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "          ********************************************************************************\n",
            "\n",
            "  !!\n",
            "    self.initialize_options()\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/local_product\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/local_product/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/local_product\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/local_product/local_product_cpu.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/local_product\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/local_product/local_product_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/local_product\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/events\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/events/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/events\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/events/filters.py -> build/bdist.linux-x86_64/wheel/fast_transformers/events\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/events/event.py -> build/bdist.linux-x86_64/wheel/fast_transformers/events\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/events/event_dispatcher.py -> build/bdist.linux-x86_64/wheel/fast_transformers/events\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/utils.py -> build/bdist.linux-x86_64/wheel/fast_transformers\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/hashing\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cpu.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/hashing\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/hashing/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/hashing\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/hashing\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention/full_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention/improved_clustered_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention/local_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention/linear_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention/reformer_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention/conditional_full_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention/causal_linear_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention/exact_topk_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention/attention_layer.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention/clustered_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention/improved_clustered_causal_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/recurrent\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/recurrent/attention\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/recurrent/attention/self_attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention/full_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/recurrent/attention/self_attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention/linear_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/recurrent/attention/self_attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/recurrent/attention/self_attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention/attention_layer.py -> build/bdist.linux-x86_64/wheel/fast_transformers/recurrent/attention/self_attention\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention/full_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention/linear_attention.py -> build/bdist.linux-x86_64/wheel/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention/attention_layer.py -> build/bdist.linux-x86_64/wheel/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/recurrent/attention\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/_utils.py -> build/bdist.linux-x86_64/wheel/fast_transformers/recurrent\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/recurrent\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/transformers.py -> build/bdist.linux-x86_64/wheel/fast_transformers/recurrent\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/masking.py -> build/bdist.linux-x86_64/wheel/fast_transformers\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/causal_product\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/causal_product/causal_product_cpu.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/causal_product\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/causal_product/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/causal_product\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/causal_product/causal_product_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/causal_product\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/weight_mapper.py -> build/bdist.linux-x86_64/wheel/fast_transformers\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/clustering\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/clustering/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/clustering\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/clustering/hamming\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/clustering/hamming\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/clustering/hamming\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cpu.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/clustering/hamming\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/sparse_product\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product/clustered_sparse_product_cpu.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/sparse_product\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/sparse_product\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product/clustered_sparse_product_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/sparse_product\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cpu.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/sparse_product\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/sparse_product\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/aggregate\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cpu.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/aggregate\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/aggregate\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate/clustered_aggregate_cuda.cpython-310-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/wheel/fast_transformers/aggregate\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/aggregate\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/attention_registry\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention_registry/spec.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention_registry\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention_registry/registry.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention_registry\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/attention_registry/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/attention_registry\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/builders\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/builders/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/builders\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/builders/base.py -> build/bdist.linux-x86_64/wheel/fast_transformers/builders\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/builders/transformer_builders.py -> build/bdist.linux-x86_64/wheel/fast_transformers/builders\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/builders/attention_builders.py -> build/bdist.linux-x86_64/wheel/fast_transformers/builders\n",
            "  creating build/bdist.linux-x86_64/wheel/fast_transformers/feature_maps\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/feature_maps/__init__.py -> build/bdist.linux-x86_64/wheel/fast_transformers/feature_maps\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/feature_maps/base.py -> build/bdist.linux-x86_64/wheel/fast_transformers/feature_maps\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/feature_maps/fourier_features.py -> build/bdist.linux-x86_64/wheel/fast_transformers/feature_maps\n",
            "  copying build/lib.linux-x86_64-cpython-310/fast_transformers/transformers.py -> build/bdist.linux-x86_64/wheel/fast_transformers\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  writing pytorch_fast_transformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to pytorch_fast_transformers.egg-info/dependency_links.txt\n",
            "  writing requirements to pytorch_fast_transformers.egg-info/requires.txt\n",
            "  writing top-level names to pytorch_fast_transformers.egg-info/top_level.txt\n",
            "  reading manifest file 'pytorch_fast_transformers.egg-info/SOURCES.txt'\n",
            "  writing manifest file 'pytorch_fast_transformers.egg-info/SOURCES.txt'\n",
            "  Copying pytorch_fast_transformers.egg-info to build/bdist.linux-x86_64/wheel/pytorch_fast_transformers-0.4.0-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/pytorch_fast_transformers-0.4.0.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-vqkqdd19/pytorch_fast_transformers-0.4.0-cp310-cp310-linux_x86_64.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'fast_transformers/__init__.py'\n",
            "  adding 'fast_transformers/masking.py'\n",
            "  adding 'fast_transformers/transformers.py'\n",
            "  adding 'fast_transformers/utils.py'\n",
            "  adding 'fast_transformers/weight_mapper.py'\n",
            "  adding 'fast_transformers/aggregate/__init__.py'\n",
            "  adding 'fast_transformers/aggregate/aggregate_cpu.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/aggregate/aggregate_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/aggregate/clustered_aggregate_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/attention/__init__.py'\n",
            "  adding 'fast_transformers/attention/attention_layer.py'\n",
            "  adding 'fast_transformers/attention/causal_linear_attention.py'\n",
            "  adding 'fast_transformers/attention/clustered_attention.py'\n",
            "  adding 'fast_transformers/attention/conditional_full_attention.py'\n",
            "  adding 'fast_transformers/attention/exact_topk_attention.py'\n",
            "  adding 'fast_transformers/attention/full_attention.py'\n",
            "  adding 'fast_transformers/attention/improved_clustered_attention.py'\n",
            "  adding 'fast_transformers/attention/improved_clustered_causal_attention.py'\n",
            "  adding 'fast_transformers/attention/linear_attention.py'\n",
            "  adding 'fast_transformers/attention/local_attention.py'\n",
            "  adding 'fast_transformers/attention/reformer_attention.py'\n",
            "  adding 'fast_transformers/attention_registry/__init__.py'\n",
            "  adding 'fast_transformers/attention_registry/registry.py'\n",
            "  adding 'fast_transformers/attention_registry/spec.py'\n",
            "  adding 'fast_transformers/builders/__init__.py'\n",
            "  adding 'fast_transformers/builders/attention_builders.py'\n",
            "  adding 'fast_transformers/builders/base.py'\n",
            "  adding 'fast_transformers/builders/transformer_builders.py'\n",
            "  adding 'fast_transformers/causal_product/__init__.py'\n",
            "  adding 'fast_transformers/causal_product/causal_product_cpu.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/causal_product/causal_product_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/clustering/__init__.py'\n",
            "  adding 'fast_transformers/clustering/hamming/__init__.py'\n",
            "  adding 'fast_transformers/clustering/hamming/cluster_cpu.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/clustering/hamming/cluster_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/events/__init__.py'\n",
            "  adding 'fast_transformers/events/event.py'\n",
            "  adding 'fast_transformers/events/event_dispatcher.py'\n",
            "  adding 'fast_transformers/events/filters.py'\n",
            "  adding 'fast_transformers/feature_maps/__init__.py'\n",
            "  adding 'fast_transformers/feature_maps/base.py'\n",
            "  adding 'fast_transformers/feature_maps/fourier_features.py'\n",
            "  adding 'fast_transformers/hashing/__init__.py'\n",
            "  adding 'fast_transformers/hashing/hash_cpu.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/hashing/hash_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/local_product/__init__.py'\n",
            "  adding 'fast_transformers/local_product/local_product_cpu.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/local_product/local_product_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/recurrent/__init__.py'\n",
            "  adding 'fast_transformers/recurrent/_utils.py'\n",
            "  adding 'fast_transformers/recurrent/transformers.py'\n",
            "  adding 'fast_transformers/recurrent/attention/__init__.py'\n",
            "  adding 'fast_transformers/recurrent/attention/cross_attention/__init__.py'\n",
            "  adding 'fast_transformers/recurrent/attention/cross_attention/attention_layer.py'\n",
            "  adding 'fast_transformers/recurrent/attention/cross_attention/full_attention.py'\n",
            "  adding 'fast_transformers/recurrent/attention/cross_attention/linear_attention.py'\n",
            "  adding 'fast_transformers/recurrent/attention/self_attention/__init__.py'\n",
            "  adding 'fast_transformers/recurrent/attention/self_attention/attention_layer.py'\n",
            "  adding 'fast_transformers/recurrent/attention/self_attention/full_attention.py'\n",
            "  adding 'fast_transformers/recurrent/attention/self_attention/linear_attention.py'\n",
            "  adding 'fast_transformers/sparse_product/__init__.py'\n",
            "  adding 'fast_transformers/sparse_product/clustered_sparse_product_cpu.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/sparse_product/clustered_sparse_product_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/sparse_product/sparse_product_cpu.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'fast_transformers/sparse_product/sparse_product_cuda.cpython-310-x86_64-linux-gnu.so'\n",
            "  adding 'pytorch_fast_transformers-0.4.0.dist-info/METADATA'\n",
            "  adding 'pytorch_fast_transformers-0.4.0.dist-info/WHEEL'\n",
            "  adding 'pytorch_fast_transformers-0.4.0.dist-info/top_level.txt'\n",
            "  adding 'pytorch_fast_transformers-0.4.0.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for pytorch-fast-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-fast-transformers: filename=pytorch_fast_transformers-0.4.0-cp310-cp310-linux_x86_64.whl size=20054720 sha256=03571348c2299d85c9c40f511b20ad39400001ca0811b9d529cf2dd16eb6a0d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/6b/6d/4abca344e31b65962d8e9d6fe298a5d2b89ff448493edc0df5\n",
            "Successfully built pytorch-fast-transformers\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-fast-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch-fast-transformers-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -v pytorch-fast-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7tsd_agQsy2"
      },
      "source": [
        "Try validating freshly installed package by creating a small transformer encoder and running it on dummy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "0FNJ2mEKetE-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "uLyaAYJw-RL5"
      },
      "outputs": [],
      "source": [
        "#Load MNIST\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "batch_size = 64\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7WiGT3lc7ch"
      },
      "source": [
        "Positional Encoding: We nust have a encoding for the input tensor containg the embedding for the sequence elements and current position index in the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iSqD429QknwD"
      },
      "outputs": [],
      "source": [
        "train_x=train_x.astype('float32')/255\n",
        "test_x=test_x.astype('float32')/255\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QMe3KOexMMyd"
      },
      "outputs": [],
      "source": [
        "class ImagePositionalEncoding(torch.nn.Module):\n",
        "  def __init__(self, d_model, image_size=28):\n",
        "    super(ImagePositionalEncoding, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.conv_x = torch.nn.Conv2d(1, d_model // 2, kernel_size=3, padding=1)\n",
        "    self.conv_y = torch.nn.Conv2d(1, d_model // 2, kernel_size=3, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Assumes input x is a tensor of shape (batch_size, channels, image_size, image_size)\n",
        "    pos_x = torch.arange(0, x.shape[2], dtype=torch.float).view(1, 1, -1, 1) / (x.shape[2] - 1)\n",
        "    pos_y = torch.arange(0, x.shape[3], dtype=torch.float).view(1, 1, 1, -1) / (x.shape[3] - 1)\n",
        "    pos_x = self.conv_x(pos_x.repeat(x.shape[0], 1, 1, 1))\n",
        "    pos_y = self.conv_y(pos_y.repeat(x.shape[0], 1, 1, 1))\n",
        "    pos = torch.cat([pos_x, pos_y], dim=1)\n",
        "    return x + pos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kRU51QU4eito",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "de0daf13-c744-4887-ad99-6cdffd4ebfd0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 28 but got size 1 for tensor number 1 in the list.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8bb5e85d2a1f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpos_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePositionalEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to PyTorch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mencoded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-8c016af8186f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpos_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpos_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 28 but got size 1 for tensor number 1 in the list."
          ]
        }
      ],
      "source": [
        "\n",
        "pos_encoder = ImagePositionalEncoding(d_model=256)\n",
        "image_batch = torch.from_numpy(train_x[:batch_size]).unsqueeze(1).float()  # Convert to PyTorch tensor\n",
        "encoded_images = pos_encoder(image_batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZYctyNJAewuD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM3ShNm2MF7x",
        "outputId": "92c7eb89-4505-40c3-b1a0-771d9a4c3a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you reached here, everything works torch.Size([10, 100, 128])\n"
          ]
        }
      ],
      "source": [
        "from fast_transformers.builders import TransformerEncoderBuilder\n",
        "from fast_transformers.masking import LengthMask, TriangularCausalMask\n",
        "import torch\n",
        "\n",
        "model = TransformerEncoderBuilder.from_kwargs(\n",
        "    n_layers=4,\n",
        "    n_heads=4,\n",
        "    feed_forward_dimensions=128,\n",
        "    query_dimensions=32,\n",
        "    value_dimensions=32,\n",
        "    attention_type=\"full\" # this means normal softmax attention\n",
        ").get()\n",
        "\n",
        "x = torch.rand(\n",
        "    10,  # batch size\n",
        "    100, # sequence length\n",
        "    128  # feature dimensions\n",
        ")\n",
        "y = model(x) # calling without masks which means attend to everything\n",
        "y = model(\n",
        "    x,\n",
        "    attn_mask=TriangularCausalMask(100),   # causal masking\n",
        "    length_mask=LengthMask(torch.tensor([\n",
        "        100, 70, 60, 30, 80, 100,          # The sequence length for every\n",
        "        50, 40, 10, 20                     # sample in the batch\n",
        "    ]))\n",
        ")\n",
        "print(\"If you reached here, everything works\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autoregressive Generation for MNIST\n",
        "\n",
        "We define two modules for autoregressive image generation for general purposes. One uses a recurrent formulation(since we are trying to prove Linear Transformers are RNNS) AND the other similar to the default PyTourch implementation that just accepts the whole sequence.\n",
        "\n",
        "Both of the implementation involves wrapping a transformer with an input embedding layer and a prediction layer."
      ],
      "metadata": {
        "id": "e4ddumEVf-rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from fast_transformers.builders import RecurrentEncoderBuilder\n",
        "\n",
        "class RecurrentGenerator(torch.nn.Module):\n",
        "    class PositionalEncoding(torch.nn.Module):\n",
        "        def __init__(self, d_model, dropout=0.0, max_len=5000):\n",
        "            super(RecurrentGenerator.PositionalEncoding, self).__init__()\n",
        "            self.dropout = torch.nn.Dropout(p=dropout)\n",
        "            self.d_model = d_model\n",
        "            pe = torch.zeros(max_len, d_model)\n",
        "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "            pe[:, 0::2] = torch.sin(position * div_term)\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "            pe = pe.unsqueeze(0)\n",
        "            self.register_buffer('pe', pe)\n",
        "\n",
        "        def forward(self, x, i):\n",
        "            pos_embedding =  self.pe[0, i:i+1]\n",
        "            x = torch.cat(\n",
        "                [x, pos_embedding.expand_as(x)],\n",
        "                dim=1\n",
        "            )\n",
        "            return self.dropout(x)\n",
        "\n",
        "    def __init__(self, d_model, sequence_length, mixtures,\n",
        "                 attention_type=\"full\", n_layers=4, n_heads=4,\n",
        "                 d_query=32, dropout=0.1, softmax_temp=None,\n",
        "                 attention_dropout=0.1):\n",
        "        super(RecurrentGenerator, self).__init__()\n",
        "\n",
        "        self.pos_embedding = self.PositionalEncoding(\n",
        "            d_model//2,\n",
        "            max_len=sequence_length\n",
        "        )\n",
        "        self.value_embedding = torch.nn.Embedding(\n",
        "            256,\n",
        "            d_model//2\n",
        "        )\n",
        "        self.transformer = RecurrentEncoderBuilder.from_kwargs(\n",
        "            attention_type=attention_type,\n",
        "            n_layers=n_layers,\n",
        "            n_heads=n_heads,\n",
        "            feed_forward_dimensions=n_heads*d_query*4,\n",
        "            query_dimensions=d_query,\n",
        "            value_dimensions=d_query,\n",
        "            dropout=dropout,\n",
        "            softmax_temp=softmax_temp,\n",
        "            attention_dropout=attention_dropout\n",
        "        ).get()\n",
        "        self.predictor = torch.nn.Linear(\n",
        "            d_model,\n",
        "            mixtures * 3\n",
        "        )\n",
        "\n",
        "    def forward(self, x, i=0, memory=None):\n",
        "        x = x.view(x.shape[0])\n",
        "        x = self.value_embedding(x)\n",
        "        x = self.pos_embedding(x, i)\n",
        "        y_hat, memory = self.transformer(x, memory)\n",
        "        y_hat = self.predictor(y_hat)\n",
        "\n",
        "        return y_hat, memory\n"
      ],
      "metadata": {
        "id": "3IikL7Ynf7PY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    class PositionalEncoding(torch.nn.Module):\n",
        "        def __init__(self, d_model, dropout=0.0, max_len=5000):\n",
        "            super(Generator.PositionalEncoding, self).__init__()\n",
        "            self.dropout = torch.nn.Dropout(p=dropout)\n",
        "            self.d_model = d_model\n",
        "            pe = torch.zeros(max_len, d_model)\n",
        "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "            pe[:, 0::2] = torch.sin(position * div_term)\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "            pe = pe.unsqueeze(0)\n",
        "            self.register_buffer('pe', pe)\n",
        "\n",
        "        def forward(self, x):\n",
        "            pos_embedding =  self.pe[:, :x.size(1), :]\n",
        "            pos_embedding = torch.repeat_interleave(pos_embedding, x.shape[0], dim=0)\n",
        "            x =  torch.cat([x, pos_embedding], dim=2)\n",
        "            return self.dropout(x)\n",
        "\n",
        "    def __init__(self, d_model, sequence_length, mixtures,\n",
        "                 attention_type=\"full\", n_layers=4, n_heads=4,\n",
        "                 d_query=32, dropout=0.1, softmax_temp=None,\n",
        "                 attention_dropout=0.1):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.pos_embedding = self.PositionalEncoding(\n",
        "            d_model//2,\n",
        "            max_len=sequence_length\n",
        "        )\n",
        "        self.value_embedding = torch.nn.Embedding(\n",
        "            256,\n",
        "            d_model//2\n",
        "        )\n",
        "\n",
        "        self.transformer = TransformerEncoderBuilder.from_kwargs(\n",
        "            attention_type=attention_type,\n",
        "            n_layers=n_layers,\n",
        "            n_heads=n_heads,\n",
        "            feed_forward_dimensions=n_heads*d_query*4,\n",
        "            query_dimensions=d_query,\n",
        "            value_dimensions=d_query,\n",
        "            dropout=dropout,\n",
        "            softmax_temp=softmax_temp,\n",
        "            attention_dropout=attention_dropout\n",
        "        ).get()\n",
        "\n",
        "        hidden_size = n_heads*d_query\n",
        "        self.predictor = torch.nn.Linear(\n",
        "            hidden_size,\n",
        "            mixtures * 3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.value_embedding(x)\n",
        "        x = self.pos_embedding(x)\n",
        "        triangular_mask = TriangularCausalMask(x.shape[1], device=x.device)\n",
        "        y_hat = self.transformer(x, attn_mask=triangular_mask)\n",
        "        y_hat = self.predictor(y_hat)\n",
        "\n",
        "        return y_hat"
      ],
      "metadata": {
        "id": "RUPrP4-zf90g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ktNWXnjSgZU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "O0KgXDqVQ19u"
      },
      "outputs": [],
      "source": [
        "def sample_mol(y_hat, num_classes=256):\n",
        "    \"\"\"Sample from mixture of logistics.\n",
        "\n",
        "    y_hat: NxC where C is 3*number of logistics\n",
        "    \"\"\"\n",
        "    assert len(y_hat.shape) == 2\n",
        "\n",
        "    N = y_hat.size(0)\n",
        "    nr_mix = y_hat.size(1) // 3\n",
        "\n",
        "    probs = torch.softmax(y_hat[:, :nr_mix], dim=-1)\n",
        "    means = y_hat[:, nr_mix:2 * nr_mix]\n",
        "    scales = torch.nn.functional.elu(y_hat[:, 2*nr_mix:3*nr_mix]) + 1.0001\n",
        "\n",
        "    indices = torch.multinomial(probs, 1).squeeze()\n",
        "    batch_indices = torch.arange(N, device=probs.device)\n",
        "    mu = means[batch_indices, indices]\n",
        "    s = scales[batch_indices, indices]\n",
        "    u = torch.rand(N, device=probs.device)\n",
        "    preds = mu + s*(torch.log(u) - torch.log(1-u))\n",
        "\n",
        "    return torch.min(\n",
        "        torch.max(\n",
        "            torch.round((preds+1)/2*(num_classes-1)),\n",
        "            preds.new_zeros(1),\n",
        "        ),\n",
        "        preds.new_ones(1)*(num_classes-1)\n",
        "    ).long().view(N, 1)\n",
        "\n",
        "\n",
        "def predict_with_recurrent(model, images, n):\n",
        "    memory = None\n",
        "    y_hat = []\n",
        "    x_hat = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(n):\n",
        "            x_hat.append(images[:, i:i+1])\n",
        "            yi, memory = model(x_hat[-1], i=i, memory=memory)\n",
        "            y_hat.append(yi)\n",
        "\n",
        "        for i in range(n, images.shape[1]):\n",
        "            x_hat.append(sample_mol(y_hat[-1], 256))\n",
        "            yi, memory = model(x_hat[-1], i=i, memory=memory)\n",
        "            y_hat.append(yi)\n",
        "\n",
        "        x_hat.append(sample_mol(y_hat[-1], 256))\n",
        "        x_hat = torch.stack(x_hat, dim=1)\n",
        "\n",
        "    return x_hat\n",
        "\n",
        "\n",
        "def predict(model, images, n):\n",
        "    N, L = images.shape\n",
        "    x_hat = images.new_zeros(N, L+1, dtype=torch.long)\n",
        "    x_hat[:, :n] = images[:, :n]\n",
        "    with torch.no_grad():\n",
        "        for i in range(n, L):\n",
        "            y_hat = model(x_hat[:, :i])\n",
        "            x_hat[:, i:i+1] = sample_mol(y_hat[:,-1,:], 256)\n",
        "        x_hat[:, -1:] = sample_mol(y_hat[:,-1,:], 256)\n",
        "    return x_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Pretrained Models for MNIST\n",
        "\n",
        "Angelos Katharopoulos et. al has already trained the model. So we are just importing the model from google drive which they have already pretrain."
      ],
      "metadata": {
        "id": "ACIDWF02g4aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import requests\n",
        "\n",
        "LINEAR_MODEL = \"https://drive.google.com/uc?export=download&id=17fc94TzytTdAwNMVCE7qOg75-CWLGi_p\"\n",
        "SOFTMAX_MODEL = \"https://drive.google.com/uc?export=download&id=1L47Ode6GxCMQbVMK33_ANjCu2iA4rf8l\"\n",
        "\n",
        "linear_weights = torch.load(io.BytesIO(requests.get(LINEAR_MODEL).content))\n",
        "softmax_weights = torch.load(io.BytesIO(requests.get(SOFTMAX_MODEL).content))"
      ],
      "metadata": {
        "id": "TvC7ikDjg4KD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yYldeqJiQ9lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And Now, we can create the model and generate some images. Note that we are creating recurrent model for softmax. This means that we save all keys and values to avoid computing again which is not something easily done for every transformer implementation.\n",
        "\n",
        "On the other hand, for linear attention the state has fixed size and it is natural to implement it as a recurrent model since the Attention is causal masked.\n",
        "\n",
        "#Image Generation"
      ],
      "metadata": {
        "id": "cJTN_RjEhK2e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "UU65Jkd8Q0oA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "1e1efe13-f13d-4e31-a680-ce43edc34dee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x788dfc39e770>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAEjCAYAAACSDWOaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAm3UlEQVR4nO3de1hVdb7H8c8GZYMCG29AJCJhXtIuEymReZtMssuko83YVIecSjOkYx7Hck6TqTNRdlEr03Eeg3IyG3syTzcrbziNYkqZWWlqapRC6ZGLqICwzh8d97ATf7Bhs2DL+/U863lkfdZe68dSvn5Ze63fdliWZQkAAMAmAU09AAAA0LLQfAAAAFvRfAAAAFvRfAAAAFvRfAAAAFvRfAAAAFvRfAAAAFvRfAAAAFvRfAAAAFvRfKBG+/fvl8PhUFZWVlMPBYCfevLJJ3XBBRcoMDBQl112WVMPB80IzUcLlZWVJYfDoa1btzb1UAA0E59//rlGjx6tuLg4BQcH6/zzz9e1116r5557zut9ffDBB5o6dar69++vzMxMPfbYYzp48KAeffRRbdu2zfeDh19p1dQDQPMUFxenEydOqHXr1k09FAA22Lhxo4YMGaIuXbronnvuUXR0tPLy8pSTk6N58+YpPT3dq/2tXbtWAQEBWrx4sYKCgiRJW7du1YwZM9S1a1euhLRwNB+okcPhUHBwcFMPw+j48eNq06ZNUw8DOCf85S9/kcvl0pYtWxQREeGR/fDDD17v74cfflBISIi78QCq420X1Kimez7uvPNOhYaG6vvvv9eIESMUGhqqTp06acqUKaqsrPR4fVVVlebOnavevXsrODhYUVFRGj9+vI4ePeqx3cqVK3XDDTcoJiZGTqdTCQkJmjVr1hn7Gzx4sPr06aPc3FwNHDhQbdq00R//+MdG+/6Blmbv3r3q3bv3GY2HJEVGRrr/fOrUKc2aNUsJCQlyOp3q2rWr/vjHP6qsrMy9jcPhUGZmpkpLS+VwONy1pG/fvpKksWPHeqyX/v0zvn37dg0aNEht2rRRt27d9Prrr0uSsrOzlZSUpJCQEPXo0UOrV6/2GOOBAwd03333qUePHgoJCVGHDh10yy23aP/+/e5tLMvSkCFD1KlTJ4+Gqry8XBdffLESEhJUWlra0FOJOqD5gFcqKyuVkpKiDh066KmnntKgQYP09NNPa9GiRR7bjR8/Xn/4wx/Uv39/zZs3T2PHjtUrr7yilJQUVVRUuLfLyspSaGioJk+erHnz5ikxMVGPPPKIHnrooTOOfeTIEQ0fPlyXXXaZ5s6dqyFDhjT69wu0FHFxccrNzdWOHTuM291999165JFHdPnll2vOnDkaNGiQMjIyNGbMGPc2S5Ys0YABA+R0OrVkyRItWbJEvXr10syZMyVJ48aNc68fOHCg+3VHjx7VjTfeqKSkJM2ePVtOp1NjxozRa6+9pjFjxuj666/X448/rtLSUo0ePVolJSXu127ZskUbN27UmDFj9Oyzz+ree+/VmjVrNHjwYB0/flzST03Riy++qJMnT+ree+91v3b69On64osvlJmZqbZt2/rkfKIWFlqkzMxMS5K1ZcuWGvN9+/ZZkqzMzEz3utTUVEuSNXPmTI9tf/GLX1iJiYnur//5z39akqxXXnnFY7tVq1adsf748eNnHHv8+PFWmzZtrJMnT7rXDRo0yJJkLVy40KvvE0DdfPDBB1ZgYKAVGBhoJScnW1OnTrXef/99q7y83L3Ntm3bLEnW3Xff7fHaKVOmWJKstWvXutelpqZabdu29dhuy5YtZ9SV007/jC9dutS9bufOnZYkKyAgwMrJyXGvf//998/YT021ZNOmTZYk6+WXX/ZY/9e//tWSZP3973+3cnJyrMDAQGvSpEnmEwSf4soHvFb9NwZJGjBggL755hv318uXL5fL5dK1116rw4cPu5fExESFhoZq3bp17m1DQkLcfy4pKdHhw4c1YMAAHT9+XDt37vQ4jtPp1NixYxvpuwJatmuvvVabNm3Sr371K3322WeaPXu2UlJSdP755+t//ud/JEnvvvuuJGny5Mker/2v//ovSdI777zToDGEhoZ6XEHp0aOHIiIi1KtXLyUlJbnXn/5z9bpTvZZUVFToyJEj6tatmyIiIvTJJ594HGfcuHFKSUlRenq67rjjDiUkJOixxx5r0NjhHW44hVeCg4PVqVMnj3Xt2rXzuJdj9+7dKioq8nifuLrq77V+8cUXevjhh7V27VoVFxd7bFdUVOTx9fnnn8/Na0Aj6tu3r9544w2Vl5frs88+04oVKzRnzhyNHj1a27Zt04EDBxQQEKBu3bp5vC46OloRERE6cOBAg47fuXNnORwOj3Uul0uxsbFnrJPkUXdOnDihjIwMZWZm6vvvv5dlWe7s57VEkhYvXqyEhATt3r1bGzdu9Ghe0PhoPuCVwMDAWrepqqpSZGSkXnnllRrz081LYWGhBg0apPDwcM2cOVMJCQkKDg7WJ598ogcffFBVVVUer6M4APYICgpS37591bdvX3Xv3l1jx47V8uXL3fnPGwRfOVt9Odv66g1Genq6MjMzNWnSJCUnJ8vlcsnhcGjMmDFn1BJJWr9+vfsm2c8//1zJyck++A5QVzQf8LmEhAStXr1a/fv3NzYM69ev15EjR/TGG2943HS2b98+O4YJoA6uuOIKSdKhQ4cUFxenqqoq7d69W7169XJvU1BQoMLCQsXFxRn31VhNiyS9/vrrSk1N1dNPP+1ed/LkSRUWFp6x7aFDh5Senq5hw4YpKChIU6ZMUUpKSq3jh+9wzwd87je/+Y0qKys1a9asM7JTp065i8Hp32aq//ZSXl6uF154wZZxAvi3devWefwsnnb6Po8ePXro+uuvlyTNnTvXY5tnnnlGknTDDTcYj3H6SZKaGoKGCgwMPGP8zz333BmP7UvSPffco6qqKi1evFiLFi1Sq1atdNddd9X4/aNxcOWjhXvxxRe1atWqM9bffPPN9d7noEGDNH78eGVkZGjbtm0aNmyYWrdurd27d2v58uWaN2+eRo8erauuukrt2rVTamqq7r//fjkcDi1ZsoQCADSB9PR0HT9+XCNHjlTPnj1VXl6ujRs36rXXXlPXrl01duxYRUREKDU1VYsWLXK/bfrxxx/rpZde0ogRI2p9/D0hIUERERFauHChwsLC1LZtWyUlJSk+Pr7B47/xxhu1ZMkSuVwuXXTRRdq0aZNWr16tDh06eGyXmZmpd955R1lZWercubOkn5qU22+/XQsWLNB9993X4LGgDprwSRs0odOP2p5tOf247M8ftf35o3OWZVnTp0+3avqntGjRIisxMdEKCQmxwsLCrIsvvtiaOnWqdfDgQfc2//rXv6wrr7zSCgkJsWJiYtyP90my1q1b595u0KBBVu/evX16DgD823vvvWf9/ve/t3r27GmFhoZaQUFBVrdu3az09HSroKDAvV1FRYU1Y8YMKz4+3mrdurUVGxtrTZs2zePReMs6e71YuXKlddFFF1mtWrXyqDFn+xmPi4uzbrjhhjPWS7LS0tLcXx89etQaO3as1bFjRys0NNRKSUmxdu7cacXFxVmpqamWZVlWXl6e5XK5rJtuuumM/Y0cOdJq27at9c0339TpfKFhHJbFr5kAAMA+3PMBAABsRfMBAABsRfMBAABsRfMBAABsRfMBAABsRfMBAABs1ewmGauqqtLBgwcVFhbWqFPxAjg7y7JUUlKimJgYBQT4x+8o1A6gaXlVNxprApHnn3/eiouLs5xOp9WvXz9r8+bNdXpdXl6ecfIrFhYW+5a8vLzGKhE1qm/dsCxqBwtLc1nqUjca5crHa6+9psmTJ2vhwoVKSkrS3LlzlZKSol27dp31Y9ZPCwsLkyRdrevVSq0bY3gAanFKFfpI77p/Hu3QkLohUTuApuZN3WiUGU6TkpLUt29fPf/885J+uhwaGxur9PR0PfTQQ8bXFhcXy+VyabBuVisHBQRoCqesCq3XShUVFSk8PNyWYzakbkjUDqCpeVM3fP5mbnl5uXJzczV06NB/HyQgQEOHDtWmTZvO2L6srEzFxcUeC4CWxdu6IVE7AH/m8+bj8OHDqqysVFRUlMf6qKgo5efnn7F9RkaGXC6Xe4mNjfX1kAA0c97WDYnaAfizJr+Nfdq0aSoqKnIveXl5TT0kAH6A2gH4L5/fcNqxY0cFBgaqoKDAY31BQYGio6PP2N7pdMrpdPp6GAD8iLd1Q6J2AP7M51c+goKClJiYqDVr1rjXVVVVac2aNUpOTvb14QCcA6gbQMvSKI/aTp48WampqbriiivUr18/zZ07V6WlpRo7dmxjHA7AOYC6AbQcjdJ8/Pa3v9WPP/6oRx55RPn5+brsssu0atWqM24mA4DTqBtAy9Eo83w0BM/qA02vKeb5aChqB9C0mnSeDwAAABOaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYKtWTT0AoKV6bN/Hxvz2rXcZ8y63fO7L4aAZCAgONuZ5ky6vdR/dr99tzI9NO9+Y/3B5G2OeeNt2Y75uV3djfmHqJ8YcLYPPr3w8+uijcjgcHkvPnj19fRgA5xDqBtCyNMqVj969e2v16tX/PkgrLrAAMKNuAC1Ho/x0t2rVStHR0Y2xawDnKOoG0HI0yg2nu3fvVkxMjC644ALddttt+vbbb8+6bVlZmYqLiz0WAC2PN3VDonYA/sznzUdSUpKysrK0atUqLViwQPv27dOAAQNUUlJS4/YZGRlyuVzuJTY21tdDAtDMeVs3JGoH4M983nwMHz5ct9xyiy655BKlpKTo3XffVWFhof7xj3/UuP20adNUVFTkXvLy8nw9JADNnLd1Q6J2AP6s0e/oioiIUPfu3bVnz54ac6fTKafT2djDAOBHaqsbErUD8GeN3nwcO3ZMe/fu1R133NHYhwLsc+UltW4yfPEGY96rtfn1c35x9t/6JWmOetU6Bn/VUuvGiV9ebMw/S3++4Qcx/7NqsHmhB4350rtTjHnQMcuYhy3L8XpMaH58/rbLlClTlJ2drf3792vjxo0aOXKkAgMDdeutt/r6UADOEdQNoGXx+ZWP7777TrfeequOHDmiTp066eqrr1ZOTo46derk60MBOEdQN4CWxefNx7Jly3y9SwDnOOoG0LLwwXIAAMBWNB8AAMBWNB8AAMBWNB8AAMBWfGwkatQqPs6YV0aEmnfw5dknh5Ikq6zM2yHZqlXXLsb8qwm1/+gsC//SmN/+zc3GfMdH3Yx5vDbVOgb4lzYf7TLm3d4dX+s+AttWGPPKY7VMMFOL8MhjxnzDFS8a87se/cKYBzvMP1sTHxhszA/+pr0xP/Xtd8ZclnmeEfgGVz4AAICtaD4AAICtaD4AAICtaD4AAICtaD4AAICtaD4AAICtaD4AAICtaD4AAICtmGSshSq6/Upj7rjtR2P+0SVLjPll8yYa85jZG415U/vyoWhjvmfowlr3cflTU4x59BzzOYiX+e8A557K4mJj3v2eLTaNpP5+o+QGvX53VqI5v/Zv5h3UUlqu/sN9xjx8aY55B/AJrnwAAABb0XwAAABb0XwAAABb0XwAAABb0XwAAABb0XwAAABb0XwAAABbMc+Hnwrs0N6YX73ue2Oe1Nb8rPzg4ApjPug/Jxjzzu99ZsyrjGnj2/8X81wEO2961ph3++DeWo/Rfe4mr8YEQOo+focxv3XNtcb81fgPjfmQP5gnAvks5wJjfuqb/cYcdcOVDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuv5/nYsGGDnnzySeXm5urQoUNasWKFRowY4c4ty9L06dP1t7/9TYWFherfv78WLFigCy+80JfjPucFtmtnzHc+0t2Yv9VhtTGvkmXMu71rnsfiok8KjPmp0lJj3tiKbr/SmH+aOs+Yv1YSY8x7PXWs1jFUWuZz3JJQN1BXVnm5Mc8riWjQ/u9qb57n477Iy8w7+KZBh8f/8/rKR2lpqS699FLNnz+/xnz27Nl69tlntXDhQm3evFlt27ZVSkqKTp482eDBAvBP1A0A1Xl95WP48OEaPnx4jZllWZo7d64efvhh3XzzzZKkl19+WVFRUXrzzTc1ZsyYho0WgF+ibgCozqf3fOzbt0/5+fkaOnSoe53L5VJSUpI2bap5qumysjIVFxd7LABajvrUDYnaAfgznzYf+fn5kqSoqCiP9VFRUe7s5zIyMuRyudxLbGysL4cEoJmrT92QqB2AP2vyp12mTZumoqIi95KXl9fUQwLgB6gdgP/yafMRHR0tSSoo8HwSoqCgwJ39nNPpVHh4uMcCoOWoT92QqB2AP/Np8xEfH6/o6GitWbPGva64uFibN29WcrL5I8wBtEzUDaDl8fppl2PHjmnPnj3ur/ft26dt27apffv26tKliyZNmqQ///nPuvDCCxUfH68//elPiomJ8Ximv6ULCA6udZtJWz4y5teErDHmOyvKjHnqrMnGvPvis9/oJ0mnjGnTu+I/PzXme05VGfM5z/3GmEd+YZ4rAJ6oG6irA4+aG84vLnm+Qfsf9s90Y94tx1w74BteNx9bt27VkCFD3F9PnvzTf2KpqanKysrS1KlTVVpaqnHjxqmwsFBXX321Vq1apeA6/IcL4NxE3QBQndfNx+DBg2UZZm50OByaOXOmZs6c2aCBATh3UDcAVNfkT7sAAICWheYDAADYiuYDAADYiuYDAADYiuYDAADYyuunXVC7wAiXMT8woXet+7gmxDyPxNcV5o8av+2ZKcY8arF/z1Ox96krjfmy6GeM+T37f2XMI+f79/kBGktt8xTl//5yY16SYJ5jZ9tvzT+7UpAxXX6sgzHv/vgJY24eHXyFKx8AAMBWNB8AAMBWNB8AAMBWNB8AAMBWNB8AAMBWNB8AAMBWNB8AAMBWzPPRCI4N7mHMt098vtZ9VOnsnwAqSXdPnWzMo/7h3/NUfL2gnzHPvuEpY/5JuXmulWOTomoZweFa8toFdupkzCt//LHBxwC85Whtniej7JeXGPNWUwuM+daetdc3M/P4lh0z/1z9/bcpxrxqx5dejwi+x5UPAABgK5oPAABgK5oPAABgK5oPAABgK5oPAABgK5oPAABgK5oPAABgK+b5qIfAbvHG/Ja/vN/gY/Raf7cxT/hHToOP0ZRqO4evX2eeK+D8wDbG/Nol44x519xNxrw2AX161rrNmNdXG/MXJ4805s53t3g1JqAuKq/qbcxXv7jIppHUT2zrI8Z8z63mOX4uLL3AmFfu/sbrMcF7XPkAAAC2ovkAAAC2ovkAAAC2ovkAAAC2ovkAAAC2ovkAAAC2ovkAAAC2Yp6Pevh6QpQxf7fdAWM+eu+wWo+RcNunXo2puTlxcz9jfuQi8z+9RGeQMR+9d6gx7/rf5nk8Ajt2MOb5o7sb88emvGjMJenzk52NeWmU+Rw4az0C4L3W2/cb88tnTzTmYd9XGvOjtx4z5gEBljFv/2KoMZ/w1HJjvvP2+ca8d3yqMb9g+oXGvPKr3cYcdeP1lY8NGzbopptuUkxMjBwOh958802P/M4775TD4fBYrrvuOl+NF4Afom4AqM7r5qO0tFSXXnqp5s8/e3d53XXX6dChQ+7l1VdfbdAgAfg36gaA6rx+22X48OEaPny4cRun06no6Oh6DwrAuYW6AaC6RrnhdP369YqMjFSPHj00YcIEHTly9rn4y8rKVFxc7LEAaHm8qRsStQPwZz5vPq677jq9/PLLWrNmjZ544gllZ2dr+PDhqqys+SaljIwMuVwu9xIbG+vrIQFo5rytGxK1A/BnPn/aZcyYMe4/X3zxxbrkkkuUkJCg9evX65prrjlj+2nTpmny5Mnur4uLiykiQAvjbd2QqB2AP2v0eT4uuOACdezYUXv27KkxdzqdCg8P91gAtGy11Q2J2gH4s0af5+O7777TkSNHdN555zX2oWyz59aFxrzSqjLmkcHm5+Al6dgVfYy5tXVHrfswCezUyZgfHZpgzOf8xfwsfZ+gHGP+v1WnjHml1caY7z5iHv+VH5ca87TId4z5+8f2G/P7PrrdmEtSr2nfG/P2h8xzkbRk52LdaC4qjx415tHzNjZo/21fb9DLa/XSenNtnDanhzH/evhfjfnh908Y82HPTjXmMU817Py1FF43H8eOHfP4bWTfvn3atm2b2rdvr/bt22vGjBkaNWqUoqOjtXfvXk2dOlXdunVTSkqKTwcOwH9QNwBU53XzsXXrVg0ZMsT99en3XFNTU7VgwQJt375dL730kgoLCxUTE6Nhw4Zp1qxZcjqZrxFoqagbAKrzuvkYPHiwLOvs0+O+//77DRoQgHMPdQNAdXywHAAAsBXNBwAAsBXNBwAAsBXNBwAAsFWjz/NxLpp06Apj/kS0ef6G52Jqfw7869fXGvP5Pw4x5gEO81wjV4R+asxvCzPfAGjeu7Sw0PysfXq7A8a8trlSSg64jPnHH/zCmG9uZc5jXtlpzC88kmvMJck8kwmA+qgqKTHm3e/eas7/eq8x33OjeR4Q19B8Y66nHebccON1S8KVDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCuaDwAAYCvm+aiHrxLNMzgM/I/7jXlRQi3PgUuKvTrPmP8p/i1jPuObXxnzr4qijfkLZeZPE614s5MxL48wf4/pk14w5j2zf2/Me83+1pif+v6gMa9NZYNeDaC5uuixWubpuNEcZ1/8ujG/ISjZmFtlZeYDtBBc+QAAALai+QAAALai+QAAALai+QAAALai+QAAALai+QAAALai+QAAALZino9GEPHyJnPug2M83m6IMW911DwPRm1cteSF/2Ge52Pt/U8b870V5pk0LpxxzJg3dB4PAC3TwRtiG/T67yuPmzeoshq0/5aCKx8AAMBWNB8AAMBWNB8AAMBWNB8AAMBWNB8AAMBWNB8AAMBWNB8AAMBWzPPhpyqPHm3U/Qd2TzDm9//3P4x5qMNpzC9bNd6Yd9+1xZgDQE3y//MqY/7AhNeNeaDD/Dv5LQ//wZhHVJjnecJPvLrykZGRob59+yosLEyRkZEaMWKEdu3a5bHNyZMnlZaWpg4dOig0NFSjRo1SQUGBTwcNwL9QOwBU51XzkZ2drbS0NOXk5OjDDz9URUWFhg0bptLSUvc2DzzwgN566y0tX75c2dnZOnjwoH7961/7fOAA/Ae1A0B1Xr3tsmrVKo+vs7KyFBkZqdzcXA0cOFBFRUVavHixli5dql/+8peSpMzMTPXq1Us5OTm68sorfTdyAH6D2gGgugbdcFpUVCRJat++vSQpNzdXFRUVGjp0qHubnj17qkuXLtq0qeb3wcrKylRcXOyxADi3UTuAlq3ezUdVVZUmTZqk/v37q0+fPpKk/Px8BQUFKSIiwmPbqKgo5efn17ifjIwMuVwu9xIb27AP/QHQvFE7ANS7+UhLS9OOHTu0bNmyBg1g2rRpKioqci95eXkN2h+A5o3aAaBej9pOnDhRb7/9tjZs2KDOnTu710dHR6u8vFyFhYUev8EUFBQoOjq6xn05nU45nebHMgGcG6gdACQvmw/LspSenq4VK1Zo/fr1io+P98gTExPVunVrrVmzRqNGjZIk7dq1S99++62Sk5N9N2o0uvxrIo35mNAfjfkd+68x5t3HM49HS0LtgK/UNo/Hh1OeNOYdAkKMeY9/3mnML3j9M2NeZUxxmlfNR1pampYuXaqVK1cqLCzM/V6sy+VSSEiIXC6X7rrrLk2ePFnt27dXeHi40tPTlZyczN3qQAtG7QBQnVfNx4IFCyRJgwcP9lifmZmpO++8U5I0Z84cBQQEaNSoUSorK1NKSopeeOEFnwwWgH+idgCozuu3XWoTHBys+fPna/78+fUeFIBzC7UDQHV8sBwAALAVzQcAALAVzQcAALAVzQcAALAVzQcAALBVvWY4xbmv+OqTDXr9tnd7GfNYbWzQ/gHULDA83JhfnG3+AL7lnyUa856zS4x51dffGHNHz27GPGyheQLDt+OfM+YBMk8i1udvE41515mbjXlVVaUxR91w5QMAANiK5gMAANiK5gMAANiK5gMAANiK5gMAANiK5gMAANiK5gMAANiKeT5QI1d2sHmDwea4ovdxY/79g1cZ8/OfYB4QoD5KB/Y05g91nGvMH7v2E2P+zwHm/zZeODTEmL8av9SY1+bz8lPGfOT76ca8x58/NuYW83jYgisfAADAVjQfAADAVjQfAADAVjQfAADAVjQfAADAVjQfAADAVjQfAADAVszzgRpF/et/jfnXFSeN+bLkRcb8/jfv93pMAGoX/LZ5Hovf5d1lzAtmVRnzLYmvGvMB8R+aX19mGfPfbRhnzDu/GWjMu79ZyzwexhR24coHAACwFc0HAACwFc0HAACwFc0HAACwFc0HAACwFc0HAACwFc0HAACwlVfzfGRkZOiNN97Qzp07FRISoquuukpPPPGEevTo4d5m8ODBys7O9njd+PHjtXDhQt+MGLao/GKXMZ/U9aoG7T9UOQ16PfwLtaP5qPrsK2Pe6Vfm11+vy304mjNdqNxG3T+aB6+ufGRnZystLU05OTn68MMPVVFRoWHDhqm0tNRju3vuuUeHDh1yL7Nnz/bpoAH4F2oHgOq8uvKxatUqj6+zsrIUGRmp3NxcDRw40L2+TZs2io6O9s0IAfg9ageA6hp0z0dRUZEkqX379h7rX3nlFXXs2FF9+vTRtGnTdPz48bPuo6ysTMXFxR4LgHMbtQNo2er92S5VVVWaNGmS+vfvrz59+rjX/+53v1NcXJxiYmK0fft2Pfjgg9q1a5feeOONGveTkZGhGTNm1HcYAPwMtQOAw7Ksen3OzoQJE/Tee+/po48+UufOnc+63dq1a3XNNddoz549SkhIOCMvKytTWVmZ++vi4mLFxsZqsG5WK0fr+gwNQAOdsiq0XitVVFSk8PBwn+6b2gGcm7ypG/W68jFx4kS9/fbb2rBhg7F4SFJSUpIknbWAOJ1OOZ3O+gwDgJ+hdgCQvGw+LMtSenq6VqxYofXr1ys+Pr7W12zbtk2SdN5559VrgAD8H7UDQHVeNR9paWlaunSpVq5cqbCwMOXn50uSXC6XQkJCtHfvXi1dulTXX3+9OnTooO3bt+uBBx7QwIEDdckllzTKNwCg+aN2AKjOq3s+HA5HjeszMzN15513Ki8vT7fffrt27Nih0tJSxcbGauTIkXr44Yfr/L5xcXGxXC4X79sCTcjX93xQO4BzX6Pd81FbnxIbG3vGDIUAQO0AUB2f7QIAAGxF8wEAAGxF8wEAAGxF8wEAAGxF8wEAAGxF8wEAAGxF8wEAAGxF8wEAAGxF8wEAAGxF8wEAAGxF8wEAAGxF8wEAAGzl1QfL2eH0B1CdUoVU58/bBeBLp1QhqfYPhGtOqB1A0/KmbjS75qOkpESS9JHebeKRACgpKZHL5WrqYdQJtQNoHupSNxxWM/vVpqqqSgcPHlRYWJgcDoeKi4sVGxurvLw8hYeHN/Xw/BLnsGFa4vmzLEslJSWKiYlRQIB/vDtL7fAtzl/DtbRz6E3daHZXPgICAtS5c+cz1oeHh7eIv7zGxDlsmJZ2/vzlisdp1I7GwflruJZ0DutaN/zjVxoAAHDOoPkAAAC2avbNh9Pp1PTp0+V0Opt6KH6Lc9gwnD//xN9bw3D+Go5zeHbN7oZTAABwbmv2Vz4AAMC5heYDAADYiuYDAADYiuYDAADYiuYDAADYqtk3H/Pnz1fXrl0VHByspKQkffzxx009pGZrw4YNuummmxQTEyOHw6E333zTI7csS4888ojOO+88hYSEaOjQodq9e3fTDLYZysjIUN++fRUWFqbIyEiNGDFCu3bt8tjm5MmTSktLU4cOHRQaGqpRo0apoKCgiUaMs6Fu1B11o2GoG/XTrJuP1157TZMnT9b06dP1ySef6NJLL1VKSop++OGHph5as1RaWqpLL71U8+fPrzGfPXu2nn32WS1cuFCbN29W27ZtlZKSopMnT9o80uYpOztbaWlpysnJ0YcffqiKigoNGzZMpaWl7m0eeOABvfXWW1q+fLmys7N18OBB/frXv27CUePnqBveoW40DHWjnqxmrF+/flZaWpr768rKSismJsbKyMhowlH5B0nWihUr3F9XVVVZ0dHR1pNPPuleV1hYaDmdTuvVV19tghE2fz/88IMlycrOzrYs66fz1bp1a2v58uXubb766itLkrVp06amGiZ+hrpRf9SNhqNu1E2zvfJRXl6u3NxcDR061L0uICBAQ4cO1aZNm5pwZP5p3759ys/P9zifLpdLSUlJnM+zKCoqkiS1b99ekpSbm6uKigqPc9izZ0916dKFc9hMUDd8i7rhPepG3TTb5uPw4cOqrKxUVFSUx/qoqCjl5+c30aj81+lzxvmsm6qqKk2aNEn9+/dXnz59JP10DoOCghQREeGxLeew+aBu+BZ1wzvUjbpr1dQDAJqjtLQ07dixQx999FFTDwWAn6Bu1F2zvfLRsWNHBQYGnnFHcEFBgaKjo5toVP7r9DnjfNZu4sSJevvtt7Vu3Tp17tzZvT46Olrl5eUqLCz02J5z2HxQN3yLulF31A3vNNvmIygoSImJiVqzZo17XVVVldasWaPk5OQmHJl/io+PV3R0tMf5LC4u1ubNmzmf/8+yLE2cOFErVqzQ2rVrFR8f75EnJiaqdevWHudw165d+vbbbzmHzQR1w7eoG7WjbtRTU9/xarJs2TLL6XRaWVlZ1pdffmmNGzfOioiIsPLz85t6aM1SSUmJ9emnn1qffvqpJcl65plnrE8//dQ6cOCAZVmW9fjjj1sRERHWypUrre3bt1s333yzFR8fb504caKJR948TJgwwXK5XNb69eutQ4cOuZfjx4+7t7n33nutLl26WGvXrrW2bt1qJScnW8nJyU04avwcdcM71I2GoW7UT7NuPizLsp577jmrS5cuVlBQkNWvXz8rJyenqYfUbK1bt86SdMaSmppqWdZPj8396U9/sqKioiyn02ldc8011q5du5p20M1ITedOkpWZmene5sSJE9Z9991ntWvXzmrTpo01cuRI69ChQ003aNSIulF31I2GoW7Uj8OyLMu+6ywAAKCla7b3fAAAgHMTzQcAALAVzQcAALAVzQcAALAVzQcAALAVzQcAALAVzQcAALAVzQcAALAVzQcAALAVzQcAALAVzQcAALDV/wGf4AViGuZUMAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "linear = RecurrentGenerator(256, 783, 10, \"linear\", 8, 8)\n",
        "linear.load_state_dict(linear_weights)\n",
        "linear.eval()\n",
        "full = RecurrentGenerator(256, 783, 10, \"full\", 8, 8)\n",
        "full.load_state_dict(softmax_weights)\n",
        "full.eval()\n",
        "\n",
        "\n",
        "images_linear = predict_with_recurrent(linear, torch.zeros(1, 783, dtype=torch.int64), 1)\n",
        "images_full = predict_with_recurrent(full, torch.zeros(1, 783, dtype=torch.int64), 1)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].set_title(\"Linear\")\n",
        "ax[0].imshow(images_linear[0].cpu().numpy().reshape(28, 28))\n",
        "ax[1].set_title(\"Softmax\")\n",
        "ax[1].imshow(images_full[0].cpu().numpy().reshape(28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.zeros(1, 783, dtype=torch.int64).shape)\n",
        "print(images_linear.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl3n28M-dNlQ",
        "outputId": "4d21c7c9-2eb9-4f9f-a68c-8a00cba18602"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 783])\n",
            "torch.Size([1, 784, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Image Completion\n",
        "\n",
        "We will sample any 5 numbers from 0-10 and access the image corresponding to it and give it as a input to predict_with_recurrent to check if it can regenerate the same image given half of the information about the image. We choose 0,1,2,3,4 to test on experiment around the occulated data."
      ],
      "metadata": {
        "id": "Q7xbNysXhugL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code takes the dataset for the num 0,1,2,3,4 and visualises the dataset for the same"
      ],
      "metadata": {
        "id": "oCa2df680ugj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "target_digits=[0,1,2,3,4]\n",
        "digit_dataloaders = {digit: None for digit in target_digits}\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "for digit in target_digits:\n",
        "  # Filter data for the current digit\n",
        "  digit_indices = (train_dataset.targets == digit).nonzero()[:, 0]\n",
        "  digit_dataset_subset = torch.utils.data.Subset(train_dataset, digit_indices)\n",
        "\n",
        "  # Create a DataLoader for the filtered data\n",
        "  digit_dataloader = DataLoader(digit_dataset_subset, batch_size=64, shuffle=True)\n",
        "\n",
        "  digit_dataloaders[digit]=digit_dataloader\n",
        "\n",
        "dataset=[]\n",
        "for digit in target_digits:\n",
        "    images, labels = next(iter(digit_dataloaders[digit]))\n",
        "    dataset.append(images)\n",
        "\n",
        "\n",
        "# Visualize a few samples from each batch of images\n",
        "fig, axes = plt.subplots(1, len(dataset), figsize=(12, 3))  # Adjust figsize as needed\n",
        "\n",
        "for i, images in enumerate(dataset):\n",
        "  # Visualize the first image in the batch\n",
        "  for j in range(1):\n",
        "    axes[i].imshow(images[j].squeeze(), cmap='gray')\n",
        "    # Add labels for better clarity\n",
        "    axes[i].set_title(f\"Batch of Images for Digit {target_digits[i]}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "KCR1vA5kis-L",
        "outputId": "dc1813f9-472e-4beb-db73-b0230ab21381"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create occulated images of the dataset. We will now only store information about half of this image. or say, stop the data visualisation at just half."
      ],
      "metadata": {
        "id": "gEDeUY7B1BHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def occlude_half_image(image):\n",
        "  \"\"\"\n",
        "  Reduces image dimension by half by slicing (top half by default).\n",
        "\n",
        "  Args:\n",
        "      image: A PyTorch tensor representing the image (channels, height, width).\n",
        "\n",
        "  Returns:\n",
        "      A PyTorch tensor representing the half-sized image.\n",
        "  \"\"\"\n",
        "\n",
        "  # Get image dimensions\n",
        "  channels, height, width = image.shape\n",
        "\n",
        "  # Select half of the image (top half by default)\n",
        "  half_image = image[:, :height // 2, :]  # Modify slicing for bottom half\n",
        "\n",
        "  return half_image\n",
        "# def occlude_half_image(image):\n",
        "#     \"\"\"\n",
        "#     Ocludes half of the image (top half by default).\n",
        "\n",
        "#     Args:\n",
        "#         image: A PyTorch tensor representing the image (channels, height, width).\n",
        "\n",
        "#     Returns:\n",
        "#         A PyTorch tensor representing the occluded image.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Get image height\n",
        "#     image_size = image.shape[1]\n",
        "\n",
        "#     # Create a mask with the same size as the image\n",
        "#     mask = torch.zeros_like(image)\n",
        "\n",
        "#     # Set half of the mask to 1 (top half by default)\n",
        "#     mask[:, :image_size // 2] = 1\n",
        "\n",
        "#     # Apply the mask to occlude half of the image\n",
        "#     return image * mask\n"
      ],
      "metadata": {
        "id": "FNGIwsQUsCVX"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have the `dataset` list containing image tensors\n",
        "\n",
        "# Visualize a few samples from each batch of images\n",
        "fig, axes = plt.subplots(1, len(dataset), figsize=(12, 3))  # Adjust figsize as needed\n",
        "\n",
        "for i, images in enumerate(dataset):\n",
        "    # Occlude the first image (or modify the index)\n",
        "    occluded_image = occlude_half_image(images[0])\n",
        "\n",
        "    # Visualize the occluded image\n",
        "    axes[i].imshow(occluded_image.squeeze(), cmap='gray')\n",
        "    axes[i].set_title(f\"Occluded Image (Digit {target_digits[i]})\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "YG2T9uTR1vic",
        "outputId": "db722299-576a-451c-f3ea-3ba8efc7bb7d"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "occluded_dataset = []\n",
        "\n",
        "for images in dataset:\n",
        "    occluded_image = occlude_half_image(images[0])\n",
        "    occluded_dataset.append(occluded_image)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "x2QOKx-A21Iu"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now test the model"
      ],
      "metadata": {
        "id": "lbP2V6gG3BOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_image_for_model(image):\n",
        "  \"\"\"\n",
        "  Flattens the image tensor for model input.\n",
        "\n",
        "  Args:\n",
        "      image: A PyTorch tensor representing the image (channels, height, width).\n",
        "\n",
        "  Returns:\n",
        "      A PyTorch tensor representing the flattened image (channels * height * width).\n",
        "  \"\"\"\n",
        "\n",
        "  # Assuming the image has a single channel (modify if channels > 1)\n",
        "  return image.view(image.shape[0], -1)  # Flatten all dimensions except the first (batch size)\n"
      ],
      "metadata": {
        "id": "faFHTkuD3A39"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flatten_occluded_dataset=[]\n",
        "for images in occluded_dataset:\n",
        "  flatten_image=prepare_image_for_model(images)\n",
        "  flatten_occluded_dataset.append(flatten_image)\n",
        "  # flatten_occluded_dataset.append(flatten_image[:, :-1])\n",
        "print(flatten_occluded_dataset[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YACqu6gB41Vq",
        "outputId": "4d392101-9483-4e13-bfb2-21f8c31ea992"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 392])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TZVRwEbL3SV4qbqAEnMT2iKPsPf4m_H2",
      "authorship_tag": "ABX9TyNi9qPY1B9A0TCSPBpszhjc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}