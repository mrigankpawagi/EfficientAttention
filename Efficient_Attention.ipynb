{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrigankpawagi/EfficientAttention-TermPaper/blob/main/Efficient_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT2uXIBELQ0f"
      },
      "source": [
        "Contributors: Rohit Jorige, Mrigank Pawagi, Nagasai, Sahil Chaudhary\n",
        "\n",
        "**Efficient Attention**\n",
        "\n",
        "The dominant sequence transduction models are based on complex recurrent or convulational neural networks that include an encoder and decoder. Vashwani et. al. proposed new simple network architecture, the Transformer based solely on attention mechanism dispensing with recurrence and convulations entirely. There is a scope of improvement on the mechanism proposed by vaswant et. al.\n",
        "\n",
        "Attention is a very important mechanism introduce in the field of LLMs and understanding the improvement in attention is very important. The time complexity of simple attention mechanism is quadratic in nature and its a problematic time complexity for a very large number of data. Thus, Approv vyas et. al. proposed a linear transformer for the same introducing kernels to similarity function.\n",
        "\n",
        "This Google Collab includes the experiments done around\n",
        "1. Attention\n",
        "2. Linear Transformer\n",
        "3. Improving attention using LoRA\n",
        "4. (And others???)\n",
        "\n",
        "We have tried various experiments around MNIST, cifar-10 and compare the time required to perform forward and backward pass for variety of sequence lengths and batch sizes.\n",
        "\n",
        "**Credits**\n",
        "1. https://github.com/idiap/fast-transformers?tab=readme-ov-file\n",
        "2. https://github.com/lucidrains/linear-attention-transformer\n",
        "3. https://github.com/EugenHotaj/pytorch-generative/blob/master/pytorch_generative/nn/attention.py\n",
        "4. https://github.com/davidsvy/cosformer-pytorch\n",
        "5. https://github.com/kyle-gao/TF_Transformer\n",
        "\n",
        "**Papers Followed:**\n",
        "1. Attention is all you need: https://arxiv.org/pdf/1706.03762.pdf\n",
        "2. Transformers are RNNs: https: //arxiv.org/pdf/2006.16236.pdf\n",
        "3. LoRA: https://arxiv.org/pdf/2106.09685.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0mo0btDOnOs"
      },
      "source": [
        "# Installation and First Steps\n",
        "The installation is directly from PyPI. This will take several minutes since it compiles several custom CUDA kernels, not only for linear autoregressive attention. Maybe grab a coffee (if you are into these things)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSW2PtZoLPsv",
        "outputId": "937bbf5d-918d-4618-f588-96b5eba1c3b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting pytorch-fast-transformers\n",
            "  Downloading pytorch-fast-transformers-0.4.0.tar.gz (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.6/93.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command python setup.py egg_info\n",
            "  No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "  running egg_info\n",
            "  creating /tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info\n",
            "  writing /tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/dependency_links.txt\n",
            "  writing requirements to /tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/SOURCES.txt'\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:500: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  reading manifest file '/tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/SOURCES.txt'\n",
            "  writing manifest file '/tmp/pip-pip-egg-info-imkx4gxj/pytorch_fast_transformers.egg-info/SOURCES.txt'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch-fast-transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-fast-transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->pytorch-fast-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pytorch-fast-transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->pytorch-fast-transformers) (1.3.0)\n",
            "Building wheels for collected packages: pytorch-fast-transformers\n",
            "  Running command python setup.py bdist_wheel\n",
            "  No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "  running bdist_wheel\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:500: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "    warnings.warn(msg.format('we could not find ninja.'))\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib.linux-x86_64-cpython-310\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers\n",
            "  copying fast_transformers/utils.py -> build/lib.linux-x86_64-cpython-310/fast_transformers\n",
            "  copying fast_transformers/masking.py -> build/lib.linux-x86_64-cpython-310/fast_transformers\n",
            "  copying fast_transformers/weight_mapper.py -> build/lib.linux-x86_64-cpython-310/fast_transformers\n",
            "  copying fast_transformers/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers\n",
            "  copying fast_transformers/transformers.py -> build/lib.linux-x86_64-cpython-310/fast_transformers\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/local_product\n",
            "  copying fast_transformers/local_product/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/local_product\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/events\n",
            "  copying fast_transformers/events/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/events\n",
            "  copying fast_transformers/events/filters.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/events\n",
            "  copying fast_transformers/events/event.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/events\n",
            "  copying fast_transformers/events/event_dispatcher.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/events\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/hashing\n",
            "  copying fast_transformers/hashing/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/hashing\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/full_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/improved_clustered_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/local_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/linear_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/reformer_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/conditional_full_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/causal_linear_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/exact_topk_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/attention_layer.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/clustered_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  copying fast_transformers/attention/improved_clustered_causal_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent\n",
            "  copying fast_transformers/recurrent/_utils.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent\n",
            "  copying fast_transformers/recurrent/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent\n",
            "  copying fast_transformers/recurrent/transformers.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/causal_product\n",
            "  copying fast_transformers/causal_product/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/causal_product\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/clustering\n",
            "  copying fast_transformers/clustering/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/clustering\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product\n",
            "  copying fast_transformers/sparse_product/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate\n",
            "  copying fast_transformers/aggregate/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/attention_registry\n",
            "  copying fast_transformers/attention_registry/spec.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention_registry\n",
            "  copying fast_transformers/attention_registry/registry.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention_registry\n",
            "  copying fast_transformers/attention_registry/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/attention_registry\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/builders\n",
            "  copying fast_transformers/builders/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/builders\n",
            "  copying fast_transformers/builders/base.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/builders\n",
            "  copying fast_transformers/builders/transformer_builders.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/builders\n",
            "  copying fast_transformers/builders/attention_builders.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/builders\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/feature_maps\n",
            "  copying fast_transformers/feature_maps/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/feature_maps\n",
            "  copying fast_transformers/feature_maps/base.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/feature_maps\n",
            "  copying fast_transformers/feature_maps/fourier_features.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/feature_maps\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention\n",
            "  copying fast_transformers/recurrent/attention/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention\n",
            "  copying fast_transformers/recurrent/attention/self_attention/full_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention\n",
            "  copying fast_transformers/recurrent/attention/self_attention/linear_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention\n",
            "  copying fast_transformers/recurrent/attention/self_attention/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention\n",
            "  copying fast_transformers/recurrent/attention/self_attention/attention_layer.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/self_attention\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying fast_transformers/recurrent/attention/cross_attention/full_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying fast_transformers/recurrent/attention/cross_attention/linear_attention.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying fast_transformers/recurrent/attention/cross_attention/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention\n",
            "  copying fast_transformers/recurrent/attention/cross_attention/attention_layer.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/recurrent/attention/cross_attention\n",
            "  creating build/lib.linux-x86_64-cpython-310/fast_transformers/clustering/hamming\n",
            "  copying fast_transformers/clustering/hamming/__init__.py -> build/lib.linux-x86_64-cpython-310/fast_transformers/clustering/hamming\n",
            "  running build_ext\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:415: UserWarning: The detected CUDA version (12.2) has a minor version mismatch with the version that was used to compile PyTorch (12.1). Most likely this shouldn't be a problem.\n",
            "    warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "  /usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:425: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.2\n",
            "    warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "  building 'fast_transformers.hashing.hash_cpu' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/hashing\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/hashing/hash_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=hash_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.aggregate.aggregate_cpu' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/aggregate/aggregate_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=aggregate_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.clustering.hamming.cluster_cpu' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/clustering\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/clustering/hamming\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/clustering/hamming/cluster_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=cluster_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.sparse_product.sparse_product_cpu' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/sparse_product/sparse_product_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=sparse_product_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.sparse_product.clustered_sparse_product_cpu' extension\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/sparse_product/clustered_sparse_product_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/clustered_sparse_product_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=clustered_sparse_product_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/clustered_sparse_product_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product/clustered_sparse_product_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.causal_product.causal_product_cpu' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/causal_product\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/causal_product/causal_product_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/causal_product/causal_product_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=causal_product_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/causal_product/causal_product_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/causal_product/causal_product_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.local_product.local_product_cpu' extension\n",
            "  creating build/temp.linux-x86_64-cpython-310/fast_transformers/local_product\n",
            "  x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fast_transformers/local_product/local_product_cpu.cpp -o build/temp.linux-x86_64-cpython-310/fast_transformers/local_product/local_product_cpu.o -fopenmp -ffast-math -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=local_product_cpu -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/local_product/local_product_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fast_transformers/local_product/local_product_cpu.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.hashing.hash_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/hashing/hash_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=hash_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/hashing/hash_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.aggregate.aggregate_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/aggregate/aggregate_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=aggregate_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate/aggregate_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.aggregate.clustered_aggregate_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/aggregate/clustered_aggregate_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate/clustered_aggregate_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=clustered_aggregate_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  fast_transformers/aggregate/clustered_aggregate_cuda.cu(43): warning #177-D: variable \"e_idx\" was declared but never referenced\n",
            "        int e_idx = threadIdx.y;\n",
            "            ^\n",
            "\n",
            "  Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/aggregate/clustered_aggregate_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/aggregate/clustered_aggregate_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.clustering.hamming.cluster_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/clustering/hamming/cluster_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=cluster_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/clustering/hamming/cluster_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.sparse_product.sparse_product_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/sparse_product/sparse_product_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=sparse_product_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "  fast_transformers/sparse_product/sparse_product_cuda.cu(272): warning #177-D: variable \"max_threads\" was declared but never referenced\n",
            "        int max_threads = 1024;\n",
            "            ^\n",
            "\n",
            "  Remark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "  x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cuda.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-310/fast_transformers/sparse_product/sparse_product_cuda.cpython-310-x86_64-linux-gnu.so\n",
            "  building 'fast_transformers.sparse_product.clustered_sparse_product_cuda' extension\n",
            "  /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.10 -c fast_transformers/sparse_product/clustered_sparse_product_cuda.cu -o build/temp.linux-x86_64-cpython-310/fast_transformers/sparse_product/clustered_sparse_product_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -arch=compute_50 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=clustered_sparse_product_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n"
          ]
        }
      ],
      "source": [
        "!pip install -v pytorch-fast-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7tsd_agQsy2"
      },
      "source": [
        "Try validating freshly installed package by creating a small transformer encoder and running it on dummy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FNJ2mEKetE-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLyaAYJw-RL5",
        "outputId": "4f7dce4d-fec8-4efb-d0e0-b9ace56ddc69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 174677587.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 20317962.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 43021046.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 16022311.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "[[[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]\n",
            "\n",
            " [[0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  ...\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]\n",
            "  [0 0 0 ... 0 0 0]]]\n",
            "[7 2 1 ... 4 5 6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ]
        }
      ],
      "source": [
        "#Load MNIST\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "batch_size = 64\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
        ")\n",
        "train_x = train_dataset.train_data.numpy()\n",
        "train_y = train_dataset.train_labels.numpy()\n",
        "test_x = test_dataset.train_data.numpy()\n",
        "train_y = test_dataset.train_labels.numpy()\n",
        "print(train_x)\n",
        "print(train_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7WiGT3lc7ch"
      },
      "source": [
        "Positional Encoding: We nust have a encoding for the input tensor containg the embedding for the sequence elements and current position index in the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSqD429QknwD"
      },
      "outputs": [],
      "source": [
        "train_x=train_x.astype('float32')/255\n",
        "test_x=test_x.astype('float32')/255\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMe3KOexMMyd"
      },
      "outputs": [],
      "source": [
        "class ImagePositionalEncoding(torch.nn.Module):\n",
        "  def __init__(self, d_model, image_size=28):\n",
        "    super(ImagePositionalEncoding, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.conv_x = torch.nn.Conv2d(1, d_model // 2, kernel_size=3, padding=1)\n",
        "    self.conv_y = torch.nn.Conv2d(1, d_model // 2, kernel_size=3, padding=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Assumes input x is a tensor of shape (batch_size, channels, image_size, image_size)\n",
        "    pos_x = torch.arange(0, x.shape[2], dtype=torch.float).view(1, 1, -1, 1) / (x.shape[2] - 1)\n",
        "    pos_y = torch.arange(0, x.shape[3], dtype=torch.float).view(1, 1, 1, -1) / (x.shape[3] - 1)\n",
        "    pos_x = self.conv_x(pos_x.repeat(x.shape[0], 1, 1, 1))\n",
        "    pos_y = self.conv_y(pos_y.repeat(x.shape[0], 1, 1, 1))\n",
        "    pos = torch.cat([pos_x, pos_y], dim=1)\n",
        "    return x + pos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRU51QU4eito",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "de0daf13-c744-4887-ad99-6cdffd4ebfd0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Sizes of tensors must match except in dimension 1. Expected size 28 but got size 1 for tensor number 1 in the list.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8bb5e85d2a1f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpos_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePositionalEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to PyTorch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mencoded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-8c016af8186f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpos_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mpos_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 28 but got size 1 for tensor number 1 in the list."
          ]
        }
      ],
      "source": [
        "\n",
        "pos_encoder = ImagePositionalEncoding(d_model=256)\n",
        "image_batch = torch.from_numpy(train_x[:batch_size]).unsqueeze(1).float()  # Convert to PyTorch tensor\n",
        "encoded_images = pos_encoder(image_batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYctyNJAewuD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM3ShNm2MF7x",
        "outputId": "2e58a6fd-6506-4b52-9afe-6077c16579dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "If you reached here, everything works torch.Size([10, 100, 128])\n"
          ]
        }
      ],
      "source": [
        "from fast_transformers.builders import TransformerEncoderBuilder\n",
        "from fast_transformers.masking import LengthMask, TriangularCausalMask\n",
        "import torch\n",
        "\n",
        "model = TransformerEncoderBuilder.from_kwargs(\n",
        "    n_layers=4,\n",
        "    n_heads=4,\n",
        "    feed_forward_dimensions=128,\n",
        "    query_dimensions=32,\n",
        "    value_dimensions=32,\n",
        "    attention_type=\"full\" # this means normal softmax attention\n",
        ").get()\n",
        "\n",
        "x = torch.rand(\n",
        "    10,  # batch size\n",
        "    100, # sequence length\n",
        "    128  # feature dimensions\n",
        ")\n",
        "y = model(x) # calling without masks which means attend to everything\n",
        "y = model(\n",
        "    x,\n",
        "    attn_mask=TriangularCausalMask(100),   # causal masking\n",
        "    length_mask=LengthMask(torch.tensor([\n",
        "        100, 70, 60, 30, 80, 100,          # The sequence length for every\n",
        "        50, 40, 10, 20                     # sample in the batch\n",
        "    ]))\n",
        ")\n",
        "print(\"If you reached here, everything works\", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Autoregressive Generation for MNIST\n",
        "\n",
        "We define two modules for autoregressive image generation for general purposes. One uses a recurrent formulation(since we are trying to prove Linear Transformers are RNNS) AND the other similar to the default PyTourch implementation that just accepts the whole sequence.\n",
        "\n",
        "Both of the implementation involves wrapping a transformer with an input embedding layer and a prediction layer."
      ],
      "metadata": {
        "id": "e4ddumEVf-rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from fast_transformers.builders import RecurrentEncoderBuilder\n",
        "\n",
        "class RecurrentGenerator(torch.nn.Module):\n",
        "    class PositionalEncoding(torch.nn.Module):\n",
        "        def __init__(self, d_model, dropout=0.0, max_len=5000):\n",
        "            super(RecurrentGenerator.PositionalEncoding, self).__init__()\n",
        "            self.dropout = torch.nn.Dropout(p=dropout)\n",
        "            self.d_model = d_model\n",
        "            pe = torch.zeros(max_len, d_model)\n",
        "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "            pe[:, 0::2] = torch.sin(position * div_term)\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "            pe = pe.unsqueeze(0)\n",
        "            self.register_buffer('pe', pe)\n",
        "\n",
        "        def forward(self, x, i):\n",
        "            pos_embedding =  self.pe[0, i:i+1]\n",
        "            x = torch.cat(\n",
        "                [x, pos_embedding.expand_as(x)],\n",
        "                dim=1\n",
        "            )\n",
        "            return self.dropout(x)\n",
        "\n",
        "    def __init__(self, d_model, sequence_length, mixtures,\n",
        "                 attention_type=\"full\", n_layers=4, n_heads=4,\n",
        "                 d_query=32, dropout=0.1, softmax_temp=None,\n",
        "                 attention_dropout=0.1):\n",
        "        super(RecurrentGenerator, self).__init__()\n",
        "\n",
        "        self.pos_embedding = self.PositionalEncoding(\n",
        "            d_model//2,\n",
        "            max_len=sequence_length\n",
        "        )\n",
        "        self.value_embedding = torch.nn.Embedding(\n",
        "            256,\n",
        "            d_model//2\n",
        "        )\n",
        "        self.transformer = RecurrentEncoderBuilder.from_kwargs(\n",
        "            attention_type=attention_type,\n",
        "            n_layers=n_layers,\n",
        "            n_heads=n_heads,\n",
        "            feed_forward_dimensions=n_heads*d_query*4,\n",
        "            query_dimensions=d_query,\n",
        "            value_dimensions=d_query,\n",
        "            dropout=dropout,\n",
        "            softmax_temp=softmax_temp,\n",
        "            attention_dropout=attention_dropout\n",
        "        ).get()\n",
        "        self.predictor = torch.nn.Linear(\n",
        "            d_model,\n",
        "            mixtures * 3\n",
        "        )\n",
        "\n",
        "    def forward(self, x, i=0, memory=None):\n",
        "        x = x.view(x.shape[0])\n",
        "        x = self.value_embedding(x)\n",
        "        x = self.pos_embedding(x, i)\n",
        "        y_hat, memory = self.transformer(x, memory)\n",
        "        y_hat = self.predictor(y_hat)\n",
        "\n",
        "        return y_hat, memory\n"
      ],
      "metadata": {
        "id": "3IikL7Ynf7PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    class PositionalEncoding(torch.nn.Module):\n",
        "        def __init__(self, d_model, dropout=0.0, max_len=5000):\n",
        "            super(Generator.PositionalEncoding, self).__init__()\n",
        "            self.dropout = torch.nn.Dropout(p=dropout)\n",
        "            self.d_model = d_model\n",
        "            pe = torch.zeros(max_len, d_model)\n",
        "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "            pe[:, 0::2] = torch.sin(position * div_term)\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "            pe = pe.unsqueeze(0)\n",
        "            self.register_buffer('pe', pe)\n",
        "\n",
        "        def forward(self, x):\n",
        "            pos_embedding =  self.pe[:, :x.size(1), :]\n",
        "            pos_embedding = torch.repeat_interleave(pos_embedding, x.shape[0], dim=0)\n",
        "            x =  torch.cat([x, pos_embedding], dim=2)\n",
        "            return self.dropout(x)\n",
        "\n",
        "    def __init__(self, d_model, sequence_length, mixtures,\n",
        "                 attention_type=\"full\", n_layers=4, n_heads=4,\n",
        "                 d_query=32, dropout=0.1, softmax_temp=None,\n",
        "                 attention_dropout=0.1):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.pos_embedding = self.PositionalEncoding(\n",
        "            d_model//2,\n",
        "            max_len=sequence_length\n",
        "        )\n",
        "        self.value_embedding = torch.nn.Embedding(\n",
        "            256,\n",
        "            d_model//2\n",
        "        )\n",
        "\n",
        "        self.transformer = TransformerEncoderBuilder.from_kwargs(\n",
        "            attention_type=attention_type,\n",
        "            n_layers=n_layers,\n",
        "            n_heads=n_heads,\n",
        "            feed_forward_dimensions=n_heads*d_query*4,\n",
        "            query_dimensions=d_query,\n",
        "            value_dimensions=d_query,\n",
        "            dropout=dropout,\n",
        "            softmax_temp=softmax_temp,\n",
        "            attention_dropout=attention_dropout\n",
        "        ).get()\n",
        "\n",
        "        hidden_size = n_heads*d_query\n",
        "        self.predictor = torch.nn.Linear(\n",
        "            hidden_size,\n",
        "            mixtures * 3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.value_embedding(x)\n",
        "        x = self.pos_embedding(x)\n",
        "        triangular_mask = TriangularCausalMask(x.shape[1], device=x.device)\n",
        "        y_hat = self.transformer(x, attn_mask=triangular_mask)\n",
        "        y_hat = self.predictor(y_hat)\n",
        "\n",
        "        return y_hat"
      ],
      "metadata": {
        "id": "RUPrP4-zf90g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ktNWXnjSgZU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0KgXDqVQ19u"
      },
      "outputs": [],
      "source": [
        "def sample_mol(y_hat, num_classes=256):\n",
        "    \"\"\"Sample from mixture of logistics.\n",
        "\n",
        "    y_hat: NxC where C is 3*number of logistics\n",
        "    \"\"\"\n",
        "    assert len(y_hat.shape) == 2\n",
        "\n",
        "    N = y_hat.size(0)\n",
        "    nr_mix = y_hat.size(1) // 3\n",
        "\n",
        "    probs = torch.softmax(y_hat[:, :nr_mix], dim=-1)\n",
        "    means = y_hat[:, nr_mix:2 * nr_mix]\n",
        "    scales = torch.nn.functional.elu(y_hat[:, 2*nr_mix:3*nr_mix]) + 1.0001\n",
        "\n",
        "    indices = torch.multinomial(probs, 1).squeeze()\n",
        "    batch_indices = torch.arange(N, device=probs.device)\n",
        "    mu = means[batch_indices, indices]\n",
        "    s = scales[batch_indices, indices]\n",
        "    u = torch.rand(N, device=probs.device)\n",
        "    preds = mu + s*(torch.log(u) - torch.log(1-u))\n",
        "\n",
        "    return torch.min(\n",
        "        torch.max(\n",
        "            torch.round((preds+1)/2*(num_classes-1)),\n",
        "            preds.new_zeros(1),\n",
        "        ),\n",
        "        preds.new_ones(1)*(num_classes-1)\n",
        "    ).long().view(N, 1)\n",
        "\n",
        "\n",
        "def predict_with_recurrent(model, images, n):\n",
        "    memory = None\n",
        "    y_hat = []\n",
        "    x_hat = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(n):\n",
        "            x_hat.append(images[:, i:i+1])\n",
        "            yi, memory = model(x_hat[-1], i=i, memory=memory)\n",
        "            y_hat.append(yi)\n",
        "\n",
        "        for i in range(n, images.shape[1]):\n",
        "            x_hat.append(sample_mol(y_hat[-1], 256))\n",
        "            yi, memory = model(x_hat[-1], i=i, memory=memory)\n",
        "            y_hat.append(yi)\n",
        "\n",
        "        x_hat.append(sample_mol(y_hat[-1], 256))\n",
        "        x_hat = torch.stack(x_hat, dim=1)\n",
        "\n",
        "    return x_hat\n",
        "\n",
        "\n",
        "def predict(model, images, n):\n",
        "    N, L = images.shape\n",
        "    x_hat = images.new_zeros(N, L+1, dtype=torch.long)\n",
        "    x_hat[:, :n] = images[:, :n]\n",
        "    with torch.no_grad():\n",
        "        for i in range(n, L):\n",
        "            y_hat = model(x_hat[:, :i])\n",
        "            x_hat[:, i:i+1] = sample_mol(y_hat[:,-1,:], 256)\n",
        "        x_hat[:, -1:] = sample_mol(y_hat[:,-1,:], 256)\n",
        "    return x_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Pretrained Models for MNIST\n",
        "\n",
        "Angelos Katharopoulos et. al has already trained the model. So we are just importing the model from google drive which they have already pretrain."
      ],
      "metadata": {
        "id": "ACIDWF02g4aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import requests\n",
        "\n",
        "LINEAR_MODEL = \"https://drive.google.com/uc?export=download&id=17fc94TzytTdAwNMVCE7qOg75-CWLGi_p\"\n",
        "SOFTMAX_MODEL = \"https://drive.google.com/uc?export=download&id=1L47Ode6GxCMQbVMK33_ANjCu2iA4rf8l\"\n",
        "\n",
        "linear_weights = torch.load(io.BytesIO(requests.get(LINEAR_MODEL).content))\n",
        "softmax_weights = torch.load(io.BytesIO(requests.get(SOFTMAX_MODEL).content))"
      ],
      "metadata": {
        "id": "TvC7ikDjg4KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And Now, we can create the model and generate some images. Note that we are creating recurrent model for softmax. This means that we save all keys and values to avoid computing again which is not something easily done for every transformer implementation.\n",
        "\n",
        "On the other hand, for linear attention the state has fixed size and it is natural to implement it as a recurrent model since the Attention is causal masked.\n",
        "\n",
        "#Following is for one dimensional vector"
      ],
      "metadata": {
        "id": "cJTN_RjEhK2e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU65Jkd8Q0oA"
      },
      "outputs": [],
      "source": [
        "linear = RecurrentGenerator(256, 783, 10, \"linear\", 8, 8)\n",
        "linear.load_state_dict(linear_weights)\n",
        "linear.eval()\n",
        "full = RecurrentGenerator(256, 783, 10, \"full\", 8, 8)\n",
        "full.load_state_dict(softmax_weights)\n",
        "full.eval()\n",
        "\n",
        "\n",
        "images_linear = predict_with_recurrent(linear, torch.zeros(1, 783, dtype=torch.int64), 1)\n",
        "images_full = predict_with_recurrent(full, torch.zeros(1, 783, dtype=torch.int64), 1)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].set_title(\"Linear\")\n",
        "ax[0].imshow(images_linear[0].cpu().numpy().reshape(28, 28))\n",
        "ax[1].set_title(\"Softmax\")\n",
        "ax[1].imshow(images_full[0].cpu().numpy().reshape(28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.zeros(1, 783, dtype=torch.int64).shape)\n",
        "print(images_linear.shape)"
      ],
      "metadata": {
        "id": "xl3n28M-dNlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Following is for occulated data\n",
        "\n",
        "We will sample any 5 numbers from 0-10 and access the image corresponding to it and give it as a input to predict_with_recurrent to check if it can regenerate the same image given half of the information about the image."
      ],
      "metadata": {
        "id": "Q7xbNysXhugL"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TZVRwEbL3SV4qbqAEnMT2iKPsPf4m_H2",
      "authorship_tag": "ABX9TyNSCDW/k2ehgJ3zK+IwBTDp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}